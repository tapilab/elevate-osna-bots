{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will\n",
    "- read our project data into a Pandas DataFrame\n",
    "- write a function to compute simple features for each row of the data frame\n",
    "- fit a LogisticRegression model to the data\n",
    "- print the top coefficients\n",
    "- compute measures of accuracy\n",
    "\n",
    "I've given you starter code below. You should:\n",
    "- First, try to get it to work with your data. It may require changing the load_data file to match the requirements of your data (e.g., what is the object you are classifying -- a tweet, a user, a news article?)\n",
    "- Second, you should add additional features to the make_features function:\n",
    "  - Be creative. It could be additional word features, or other meta data about the user, date, etc.\n",
    "- As you try out different feature combinations, print out the coefficients and accuracy scores\n",
    "- List any features that seem to improve accuracy. Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack # \"horizontal stack\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>tweets</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>label</th>\n",
       "      <th>tweets_texts</th>\n",
       "      <th>tweets_avg_urls</th>\n",
       "      <th>tweets_avg_mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>carlos_eggbot</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:36:07 +0000 201...</td>\n",
       "      <td>0</td>\n",
       "      <td>bot</td>\n",
       "      <td>'You heard me! Shoot me.', 'Junpei, you...', '...</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ecolo_ebooks</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:36:11 +0000 201...</td>\n",
       "      <td>2</td>\n",
       "      <td>bot</td>\n",
       "      <td>\"i'm not straight but 20 bucks is 20 bu\", '\"ec...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AllStarSMBot</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:36:28 +0000 201...</td>\n",
       "      <td>3</td>\n",
       "      <td>bot</td>\n",
       "      <td>\"You'll never know if you don't go\\nYou'll nev...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saionji_en</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:36:52 +0000 201...</td>\n",
       "      <td>3</td>\n",
       "      <td>bot</td>\n",
       "      <td>\"why the fuck am i banana girl? i'll never die...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KimClune</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:37:20 +0000 201...</td>\n",
       "      <td>329</td>\n",
       "      <td>bot</td>\n",
       "      <td>'Chewing rather than drinking breakfast is AWE...</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatsDogsBOT</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:38:10 +0000 201...</td>\n",
       "      <td>3</td>\n",
       "      <td>bot</td>\n",
       "      <td>'[Discussion] If I say no, that should be it. ...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bluejovanka</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:38:14 +0000 201...</td>\n",
       "      <td>47</td>\n",
       "      <td>bot</td>\n",
       "      <td>\"I'm staying in tonight watching someone with ...</td>\n",
       "      <td>37.688442</td>\n",
       "      <td>32.160804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>anittavota4</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:39:19 +0000 201...</td>\n",
       "      <td>0</td>\n",
       "      <td>bot</td>\n",
       "      <td>'RT BrettHillOwens2: #PremiosMTVMIAW #MTVBRMUS...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>justtraveluk</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:39:21 +0000 201...</td>\n",
       "      <td>11</td>\n",
       "      <td>bot</td>\n",
       "      <td>'The Top 5 Airports in the World for Departure...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rhaudiencebot</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:40:08 +0000 201...</td>\n",
       "      <td>0</td>\n",
       "      <td>bot</td>\n",
       "      <td>'GET BUTCH, BITCH!', 'HEY RIFF, WHAT DO YOU DO...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LexyintheCity</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:40:11 +0000 201...</td>\n",
       "      <td>69</td>\n",
       "      <td>bot</td>\n",
       "      <td>'A happy belated to this crazy fucking gemini ...</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>kimberlymaich</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:41:07 +0000 201...</td>\n",
       "      <td>11</td>\n",
       "      <td>bot</td>\n",
       "      <td>'Donna Miller Fry Thank you for following me!!...</td>\n",
       "      <td>31.313131</td>\n",
       "      <td>82.323232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>timotheeribeiro</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:41:23 +0000 201...</td>\n",
       "      <td>65</td>\n",
       "      <td>bot</td>\n",
       "      <td>'Pinned to mes voyages on @Pinterest: La renco...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>86.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>666dikuto</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:41:42 +0000 201...</td>\n",
       "      <td>0</td>\n",
       "      <td>bot</td>\n",
       "      <td>\"I'll fight for us till death do us part\", \"I ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GLaDOSystem</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:41:55 +0000 201...</td>\n",
       "      <td>21</td>\n",
       "      <td>bot</td>\n",
       "      <td>\"But I overcame my handicap. That's a true sto...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gotpie_ebooks</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:42:31 +0000 201...</td>\n",
       "      <td>0</td>\n",
       "      <td>bot</td>\n",
       "      <td>'Get in the pie joffrey', 'look the pie', 'LÃ≤Õö...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Pawtersimms1</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:44:14 +0000 201...</td>\n",
       "      <td>0</td>\n",
       "      <td>bot</td>\n",
       "      <td>'Tottenham vs Liverpool FREE live stream: Watc...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mikan02862611</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:45:11 +0000 201...</td>\n",
       "      <td>0</td>\n",
       "      <td>bot</td>\n",
       "      <td>\"Another hour! It's June 02, 2019 at 03:45AM\",...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>WeatherTucson</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:45:12 +0000 201...</td>\n",
       "      <td>7</td>\n",
       "      <td>bot</td>\n",
       "      <td>'Right now: Sunny and 92F. Today: Sunny. High ...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>asta_ebooks</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:45:32 +0000 201...</td>\n",
       "      <td>2</td>\n",
       "      <td>bot</td>\n",
       "      <td>'When will there be a sequel to No.6 I need to...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>taguigtwinktop</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:45:34 +0000 201...</td>\n",
       "      <td>10</td>\n",
       "      <td>bot</td>\n",
       "      <td>'Hey Mackiedave@1988(@Mackiedave19881), thank ...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>stevenboss</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:45:44 +0000 201...</td>\n",
       "      <td>16</td>\n",
       "      <td>bot</td>\n",
       "      <td>'Today 2:00 and 7:00 pm. Support The Lord‚Äôs Cu...</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>latikia</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:46:14 +0000 201...</td>\n",
       "      <td>288</td>\n",
       "      <td>bot</td>\n",
       "      <td>'ICYMI: The Spelling Bee Champs Interview That...</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>6.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>positivenagi</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:50:00 +0000 201...</td>\n",
       "      <td>2</td>\n",
       "      <td>bot</td>\n",
       "      <td>\"Take a break, you've done so much today I'm p...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>cheesucheesu</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:50:11 +0000 201...</td>\n",
       "      <td>0</td>\n",
       "      <td>bot</td>\n",
       "      <td>'down for the clown!', 'nya ha ha!', 'my aesth...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>fetisbieber</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:50:43 +0000 201...</td>\n",
       "      <td>0</td>\n",
       "      <td>bot</td>\n",
       "      <td>'RT Moomoocorner: BDBML #PremiosMTVMIAW #MTVLA...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>anittavota5</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:51:08 +0000 201...</td>\n",
       "      <td>0</td>\n",
       "      <td>bot</td>\n",
       "      <td>'RT dj_rakete: #PremiosMTVMIAW #MTVBRMUSICALAN...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>EngkoBoti</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:51:16 +0000 201...</td>\n",
       "      <td>2</td>\n",
       "      <td>bot</td>\n",
       "      <td>'get together : Ìï®Íªò Î™®Ïù¥Îã§.\\nAll the family get to...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ZikaSdv</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:51:32 +0000 201...</td>\n",
       "      <td>0</td>\n",
       "      <td>bot</td>\n",
       "      <td>'RT monica1169: BDBML #PremiosMTVMIAW #MTVBRMU...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lancxelot</td>\n",
       "      <td>[{'created_at': 'Sat Jun 01 18:51:27 +0000 201...</td>\n",
       "      <td>2</td>\n",
       "      <td>bot</td>\n",
       "      <td>'\"I\\'m very proud to be wearing this shirt eve...</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>69.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>RevolutApp</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:32:51 +0000 201...</td>\n",
       "      <td>881</td>\n",
       "      <td>human</td>\n",
       "      <td>\"@ghostraccoon87 We're sorry to hear that! üôÅ L...</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Bakari_Sellers</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:31:55 +0000 201...</td>\n",
       "      <td>1695</td>\n",
       "      <td>human</td>\n",
       "      <td>'@AsteadWesley It‚Äôs all good. I Just know ther...</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>59.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>AmazonHelp</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:33:53 +0000 201...</td>\n",
       "      <td>943</td>\n",
       "      <td>human</td>\n",
       "      <td>'@parasjain777 In that case, please share your...</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>UPSHelp</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:34:00 +0000 201...</td>\n",
       "      <td>289</td>\n",
       "      <td>human</td>\n",
       "      <td>\"@juliabullia_ I am sorry you feel this way. W...</td>\n",
       "      <td>85.500000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>fly2midway</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:34:23 +0000 201...</td>\n",
       "      <td>394</td>\n",
       "      <td>human</td>\n",
       "      <td>'Yesterday, Commissioner Jamie L. Rhee met wit...</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>WaltThurm3</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:34:44 +0000 201...</td>\n",
       "      <td>573</td>\n",
       "      <td>human</td>\n",
       "      <td>\"@AtlantaFalcons @KBDeuce4 @MattBosher5 @KBDeu...</td>\n",
       "      <td>40.201005</td>\n",
       "      <td>70.351759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>scottjohnson</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:34:59 +0000 201...</td>\n",
       "      <td>2489</td>\n",
       "      <td>human</td>\n",
       "      <td>'Just a reminder that WE pay for tariffs. Have...</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>61.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>TOIBengaluru</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:35:02 +0000 201...</td>\n",
       "      <td>400</td>\n",
       "      <td>human</td>\n",
       "      <td>'Animal assisted therapy finds more takers in ...</td>\n",
       "      <td>92.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>LASchoolPolice</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:35:32 +0000 201...</td>\n",
       "      <td>173</td>\n",
       "      <td>human</td>\n",
       "      <td>'Shelter in place will be lifted at 12:10 PM d...</td>\n",
       "      <td>74.500000</td>\n",
       "      <td>79.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>bbmayabb</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:36:41 +0000 201...</td>\n",
       "      <td>34</td>\n",
       "      <td>human</td>\n",
       "      <td>'@caisersoze84 @SaadoonMustafa So elegant .. y...</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>87.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>livexlive</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:37:02 +0000 201...</td>\n",
       "      <td>67</td>\n",
       "      <td>human</td>\n",
       "      <td>\"@halseyph Hey! Just letting you know that we‚Äô...</td>\n",
       "      <td>63.500000</td>\n",
       "      <td>85.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Its_Katka</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:36:54 +0000 201...</td>\n",
       "      <td>281</td>\n",
       "      <td>human</td>\n",
       "      <td>'Like one guy said I was ‚Äúon the way to the vi...</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>OregonGovBrown</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:37:36 +0000 201...</td>\n",
       "      <td>913</td>\n",
       "      <td>human</td>\n",
       "      <td>\"Climate change threatens our communities, our...</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>44.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>ExpressNews</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:37:40 +0000 201...</td>\n",
       "      <td>283</td>\n",
       "      <td>human</td>\n",
       "      <td>'Texas Legislature declines to expand Medicaid...</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>12.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>ChuckWendig</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:37:49 +0000 201...</td>\n",
       "      <td>4138</td>\n",
       "      <td>human</td>\n",
       "      <td>'@veschwab Tell them the VE stands for VENGEAN...</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>79.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>chrisdeville</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:37:50 +0000 201...</td>\n",
       "      <td>240</td>\n",
       "      <td>human</td>\n",
       "      <td>'A ban on all fianc√©e / Beyonc√© rhymes', 'RT @...</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>69.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>VirginTrains</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:38:14 +0000 201...</td>\n",
       "      <td>1566</td>\n",
       "      <td>human</td>\n",
       "      <td>'@aclaireporter @thetrainline Ah apologies abo...</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>UNDPAzerbaijan</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:38:20 +0000 201...</td>\n",
       "      <td>80</td>\n",
       "      <td>human</td>\n",
       "      <td>'NOW HIRING: an experienced &amp;amp; motivated #E...</td>\n",
       "      <td>57.500000</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>DrNerdLove</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:38:39 +0000 201...</td>\n",
       "      <td>228</td>\n",
       "      <td>human</td>\n",
       "      <td>'‚ÄúExcuse me, do you have the time to talk abou...</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>CharlesSoule</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:38:40 +0000 201...</td>\n",
       "      <td>617</td>\n",
       "      <td>human</td>\n",
       "      <td>'@tmalghem I‚Äôm sure there will be signed copie...</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>74.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>fordm</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:38:43 +0000 201...</td>\n",
       "      <td>1202</td>\n",
       "      <td>human</td>\n",
       "      <td>\"15 members of Britain's House of Lords have b...</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>ChadPawson</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:38:48 +0000 201...</td>\n",
       "      <td>84</td>\n",
       "      <td>human</td>\n",
       "      <td>\"Curious if there is anyone in B.C. who has no...</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>93.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>ericvdunn</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:39:18 +0000 201...</td>\n",
       "      <td>646</td>\n",
       "      <td>human</td>\n",
       "      <td>'Hurricane season at midnight. Lord let these ...</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>46.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>RossDellenger</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:39:25 +0000 201...</td>\n",
       "      <td>645</td>\n",
       "      <td>human</td>\n",
       "      <td>'In Mississippi, gaining \"resort status\" is a ...</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>40.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>MrGerryCampbell</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:39:36 +0000 201...</td>\n",
       "      <td>81</td>\n",
       "      <td>human</td>\n",
       "      <td>'Serving Police Officer arrested for a #Domest...</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>davidmweissman</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:39:59 +0000 201...</td>\n",
       "      <td>444</td>\n",
       "      <td>human</td>\n",
       "      <td>'@MiheerDodhia @sunny @ewarren @FoxNews I wrot...</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>82.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>PaulRieckhoff</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:40:02 +0000 201...</td>\n",
       "      <td>1386</td>\n",
       "      <td>human</td>\n",
       "      <td>'Blah. Nobody wants this except Ortiz who gets...</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>84.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Monster</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:40:40 +0000 201...</td>\n",
       "      <td>3638</td>\n",
       "      <td>human</td>\n",
       "      <td>\"@ENIMSAJN_ A second job? If you're looking, w...</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>sligogaa</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:40:45 +0000 201...</td>\n",
       "      <td>174</td>\n",
       "      <td>human</td>\n",
       "      <td>'U17 Connacht League Rd2\\n\\n10mins 1st half\\nS...</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>FunSizeSuze</td>\n",
       "      <td>[{'created_at': 'Fri May 31 18:40:46 +0000 201...</td>\n",
       "      <td>312</td>\n",
       "      <td>human</td>\n",
       "      <td>'@soozaphone @cosmicshambles Ooh, yes! *pulls ...</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>84.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       screen_name                                             tweets  \\\n",
       "0    carlos_eggbot  [{'created_at': 'Sat Jun 01 18:36:07 +0000 201...   \n",
       "1     ecolo_ebooks  [{'created_at': 'Sat Jun 01 18:36:11 +0000 201...   \n",
       "2     AllStarSMBot  [{'created_at': 'Sat Jun 01 18:36:28 +0000 201...   \n",
       "3       saionji_en  [{'created_at': 'Sat Jun 01 18:36:52 +0000 201...   \n",
       "4         KimClune  [{'created_at': 'Sat Jun 01 18:37:20 +0000 201...   \n",
       "..             ...                                                ...   \n",
       "95  davidmweissman  [{'created_at': 'Fri May 31 18:39:59 +0000 201...   \n",
       "96   PaulRieckhoff  [{'created_at': 'Fri May 31 18:40:02 +0000 201...   \n",
       "97         Monster  [{'created_at': 'Fri May 31 18:40:40 +0000 201...   \n",
       "98        sligogaa  [{'created_at': 'Fri May 31 18:40:45 +0000 201...   \n",
       "99     FunSizeSuze  [{'created_at': 'Fri May 31 18:40:46 +0000 201...   \n",
       "\n",
       "    listed_count  label                                       tweets_texts  \\\n",
       "0              0    bot  'You heard me! Shoot me.', 'Junpei, you...', '...   \n",
       "1              2    bot  \"i'm not straight but 20 bucks is 20 bu\", '\"ec...   \n",
       "2              3    bot  \"You'll never know if you don't go\\nYou'll nev...   \n",
       "3              3    bot  \"why the fuck am i banana girl? i'll never die...   \n",
       "4            329    bot  'Chewing rather than drinking breakfast is AWE...   \n",
       "..           ...    ...                                                ...   \n",
       "95           444  human  '@MiheerDodhia @sunny @ewarren @FoxNews I wrot...   \n",
       "96          1386  human  'Blah. Nobody wants this except Ortiz who gets...   \n",
       "97          3638  human  \"@ENIMSAJN_ A second job? If you're looking, w...   \n",
       "98           174  human  'U17 Connacht League Rd2\\n\\n10mins 1st half\\nS...   \n",
       "99           312  human  '@soozaphone @cosmicshambles Ooh, yes! *pulls ...   \n",
       "\n",
       "    tweets_avg_urls  tweets_avg_mentions  \n",
       "0              10.5                  0.0  \n",
       "1               0.0                  0.0  \n",
       "2               0.0                  0.0  \n",
       "3               0.0                  0.0  \n",
       "4              28.5                  2.5  \n",
       "..              ...                  ...  \n",
       "95             27.5                 82.0  \n",
       "96             45.5                 84.5  \n",
       "97             99.0                100.0  \n",
       "98             18.5                 34.0  \n",
       "99             37.0                 84.5  \n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(datafile):\n",
    "    \"\"\"\n",
    "    Read your data into a single pandas dataframe where\n",
    "    - each row is an instance to be classified\n",
    "    (this could be a tweet, user, or news article, depending on your project)\n",
    "    - there is a column called `label` which stores the class label (e.g., the true\n",
    "      category for this row)\n",
    "    \"\"\"\n",
    "    bots = []\n",
    "    humans = []\n",
    "    folder = ['/bots', '/humans']\n",
    "    name = '/*.json.gz'\n",
    "    for f in folder:\n",
    "        paths = glob.glob(datafile + f + name)\n",
    "        for p in paths:\n",
    "            with gzip.open(p, 'r') as file:\n",
    "                for line in file:\n",
    "                    if f == folder[0]:\n",
    "                        bots.append(json.loads(line))\n",
    "                    elif f == folder[1]:\n",
    "                        humans.append(json.loads(line))\n",
    "    df_bots = pd.DataFrame(bots)[['screen_name','tweets','listed_count']]\n",
    "    df_bots['label'] = 'bot'\n",
    "    df_humans = pd.DataFrame(humans)[['screen_name','tweets','listed_count']]\n",
    "    df_humans['label'] = 'human'\n",
    "    frames = [df_bots, df_humans]\n",
    "    df = pd.concat(frames)\n",
    "    users = bots + humans\n",
    "    tweets = [u['tweets'] for u in users]\n",
    "    text = [d['full_text'] for t in tweets for d in t] \n",
    "\n",
    "#     tweets_avg_len = []\n",
    "    tweets_avg_mentions = []\n",
    "    tweets_avg_urls = []\n",
    "    factor = 100\n",
    "    tweets_texts = []\n",
    "    for u in users:\n",
    "        tweets = u['tweets'] # a list of dicts\n",
    "        texts = [t['full_text'] for t in tweets]\n",
    "        tweets_texts.append(str(texts).strip('[]'))\n",
    "#         avg_len = sum(map(len, texts))/len(texts)\n",
    "#         tweets_avg_len.append(int(avg_len))\n",
    "        count_mention = 0\n",
    "        count_url = 0\n",
    "        for s in texts:\n",
    "            if 'http' in s:\n",
    "                count_url+=1\n",
    "            if '@' in s:\n",
    "                count_mention+=1\n",
    "        tweets_avg_urls.append(100 * count_url / len(texts))\n",
    "        tweets_avg_mentions.append(100 * count_mention / len(texts))\n",
    "#     df['tweets_avg_len'] = tweets_avg_len\n",
    "    df['tweets_texts'] = tweets_texts\n",
    "    df['tweets_avg_urls'] = tweets_avg_urls\n",
    "    df['tweets_avg_mentions'] = tweets_avg_mentions\n",
    "    return df\n",
    "# df = load_data('~/Dropbox/elevate/harassment/training_data/data.csv.gz')\n",
    "df = load_data('/Users/sheepman/Downloads/bots/small')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "screen_name             object\n",
       "tweets                  object\n",
       "listed_count             int64\n",
       "label                   object\n",
       "tweets_texts            object\n",
       "tweets_avg_urls        float64\n",
       "tweets_avg_mentions    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the distribution over class labels?\n",
    "df.label.value_counts()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(df):\n",
    "    vec = DictVectorizer()\n",
    "    feature_dicts = []\n",
    "    labels_to_track = ['tweets_avg_urls', 'tweets_avg_mentions','listed_count']\n",
    "    for i, row in df.iterrows():\n",
    "        features = {}\n",
    "        features['tweets_avg_urls'] = row['tweets_avg_urls']\n",
    "        features['tweets_avg_mentions'] = row['tweets_avg_mentions']\n",
    "        features['listed_count'] = row['listed_count']\n",
    "        feature_dicts.append(features)\n",
    "    X = vec.fit_transform(feature_dicts)\n",
    "#     print(X)\n",
    "    return X, vec\n",
    "\n",
    "X, vec = make_features(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 3)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are dimensions of the feature matrix?\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tweets_avg_urls': 2, 'tweets_avg_mentions': 1, 'listed_count': 0}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the feature names?\n",
    "# vocabulary_ is a dict from feature name to column index\n",
    "vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 68224)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use CountVectorizer to create a term feature matrix\n",
    "count_vec = CountVectorizer()\n",
    "X_words = count_vec.fit_transform(df.tweets_texts)\n",
    "X_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68224 words\n",
      "180988 of 13644800 possible cells are non-zero (1.33%)\n"
     ]
    }
   ],
   "source": [
    "# how sparse is it? \n",
    "def print_sparsity(X_words):\n",
    "    print('%d words' % X_words.shape[1])\n",
    "    num_cells = X_words.shape[0] * X_words.shape[1]\n",
    "    print('%d of %d possible cells are non-zero (%.2f%%)' %\n",
    "          (X_words.nnz, num_cells,\n",
    "           100 * X_words.nnz/num_cells))\n",
    "print_sparsity(X_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_df=1\n",
      "68224 words\n",
      "180988 of 13644800 possible cells are non-zero (1.33%)\n",
      "\n",
      "\n",
      "min_df=2\n",
      "14528 words\n",
      "127292 of 2905600 possible cells are non-zero (4.38%)\n",
      "\n",
      "\n",
      "min_df=5\n",
      "5708 words\n",
      "104659 of 1141600 possible cells are non-zero (9.17%)\n",
      "\n",
      "\n",
      "min_df=10\n",
      "2988 words\n",
      "86980 of 597600 possible cells are non-zero (14.55%)\n",
      "\n",
      "\n",
      "max_df=1.000000\n",
      "14528 words\n",
      "127292 of 2905600 possible cells are non-zero (4.38%)\n",
      "\n",
      "\n",
      "max_df=0.950000\n",
      "14528 words\n",
      "127292 of 2905600 possible cells are non-zero (4.38%)\n",
      "\n",
      "\n",
      "max_df=0.800000\n",
      "14514 words\n",
      "124925 of 2902800 possible cells are non-zero (4.30%)\n",
      "\n",
      "\n",
      "ngram= (1, 1)\n",
      "14528 words\n",
      "127292 of 2905600 possible cells are non-zero (4.38%)\n",
      "\n",
      "\n",
      "ngram= (1, 2)\n",
      "47248 words\n",
      "261312 of 9449600 possible cells are non-zero (2.77%)\n",
      "\n",
      "\n",
      "ngram= (1, 3)\n",
      "65630 words\n",
      "312700 of 13126000 possible cells are non-zero (2.38%)\n"
     ]
    }
   ],
   "source": [
    "# how does sparsity vary with min_df?\n",
    "# \n",
    "X_words_list = []\n",
    "for min_df in [1,2,5,10]:\n",
    "    count_vec = CountVectorizer(min_df=min_df, max_df=1.0, ngram_range=(1,1))\n",
    "    print('\\n\\nmin_df=%d' % min_df)\n",
    "    X_words = count_vec.fit_transform(df.tweets_texts)\n",
    "    X_words_list.append(X_words)\n",
    "    print_sparsity(X_words)\n",
    "\n",
    "for max_df in  [1.0, 0.95, 0.8]:\n",
    "    count_vec = CountVectorizer(min_df=2, max_df=max_df, ngram_range=(1,1))\n",
    "    print('\\n\\nmax_df=%f' % max_df)\n",
    "    X_words = count_vec.fit_transform(df.tweets_texts)\n",
    "    X_words_list.append(X_words)\n",
    "    print_sparsity(X_words)\n",
    "\n",
    "# terms using different ngrams\n",
    "for ngram in [(1,1), (1,2), (1,3)]:\n",
    "    count_vec = CountVectorizer(min_df=2, max_df=1.0, ngram_range=ngram)\n",
    "    print('\\n\\nngram=', str(ngram))\n",
    "    X_words = count_vec.fit_transform(df.tweets_texts)\n",
    "    X_words_list.append(X_words)\n",
    "    print_sparsity(X_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tweets_avg_urls\t9149\n",
      " tweets_avg_mentions\t7028\n",
      "        listed_count\t147695\n"
     ]
    }
   ],
   "source": [
    "# how often does each word occur?\n",
    "for word, idx in vec.vocabulary_.items():\n",
    "    print('%20s\\t%d' % (word, X[:,idx].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['listed_count', 'tweets_avg_mentions', 'tweets_avg_urls']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can also get a simple list of feature names:\n",
    "vec.get_feature_names()\n",
    "\n",
    "# e.g., first column is 'hate', second is 'love', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'bot': 100, 'human': 100})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll first store the classes separately in a numpy array\n",
    "y = np.array(df.label)\n",
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the class names\n",
    "class_names = set(df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tweets_avg_urls\t               human\t5491\n",
      "     tweets_avg_urls\t                 bot\t3657\n",
      " tweets_avg_mentions\t               human\t6123\n",
      " tweets_avg_mentions\t                 bot\t905\n",
      "        listed_count\t               human\t145344\n",
      "        listed_count\t                 bot\t2351\n"
     ]
    }
   ],
   "source": [
    "# how often does each word appear in each class?\n",
    "for word, idx in vec.vocabulary_.items():\n",
    "    for class_name in class_names:\n",
    "        class_idx = np.where(y==class_name)[0]\n",
    "        print('%20s\\t%20s\\t%d' % (word, class_name, X[class_idx, idx].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, `you` appears more frequently in positive (hostile) class, and `love` appears more frequently in the negative (non-hostile) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_dif = 1\n",
      "(200, 68224)\n",
      "min_dif = 2\n",
      "(200, 14528)\n",
      "min_dif = 5\n",
      "(200, 5708)\n",
      "min_dif = 10\n",
      "(200, 2988)\n",
      "max_dif = 1.0\n",
      "(200, 14528)\n",
      "max_dif = 0.95\n",
      "(200, 14528)\n",
      "max_dif = 0.8\n",
      "(200, 14514)\n",
      "ngram_range = (1,1)\n",
      "(200, 14528)\n",
      "ngram_range = (1,2)\n",
      "(200, 47248)\n",
      "ngram_range = (1,3)\n",
      "(200, 65630)\n"
     ]
    }
   ],
   "source": [
    "terms = [('min_dif', 1),('min_dif', 2),('min_dif', 5),('min_dif', 10),\n",
    "        ('max_dif', 1.00),('max_dif', 0.95),('max_dif', 0.80),\n",
    "        ('ngram_range', '(1,1)'), ('ngram_range', '(1,2)'),('ngram_range', '(1,3)')]\n",
    "# X_all_list = [X_words for X_words in all_X_words]\n",
    "for i in range(len(X_words_list)):\n",
    "    print(terms[i][0], '=', terms[i][1])\n",
    "    print(np.shape(X_words_list[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = LogisticRegression(C=1, penalty='l2')\n",
    "mean = []\n",
    "std = []\n",
    "arr_accuracies = []\n",
    "for idx in range(len(X_words_list)):\n",
    "    clf.fit(X_words_list[idx], y)\n",
    "#     print(clf.coef_)\n",
    "    coef = [-clf.coef_[0], clf.coef_[0]]\n",
    "#     print(coef)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    X = X_words_list[idx]\n",
    "    for train, test in kf.split(X):\n",
    "        clf.fit(X[train], y[train])\n",
    "        pred = clf.predict(X[test])\n",
    "        accuracies.append(accuracy_score(y[test], pred))\n",
    "    arr_accuracies.append(str(accuracies))\n",
    "    mean.append(np.mean(accuracies))\n",
    "    std.append(np.std(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_dif   accuracy\n",
      "    1      0.77\n",
      "    2      0.76\n",
      "    5      0.78\n",
      "   10      0.77\n",
      "max_dif   accuracy\n",
      "  1.0      0.76\n",
      " 0.95      0.76\n",
      "  0.8      0.78\n",
      "ngram   accuracy\n",
      "(1,1)      0.76\n",
      "(1,2)      0.78\n",
      "(1,3)      0.76\n"
     ]
    }
   ],
   "source": [
    "print('min_dif', '%10s'%'accuracy')\n",
    "for i in range(0,4):\n",
    "    print('%5s%10.2f'%(terms[i][1], mean[i]))\n",
    "print('max_dif', '%10s'%'accuracy')\n",
    "for i in range(4,7):\n",
    "    print('%5s%10.2f'%(terms[i][1], mean[i])) \n",
    "print('ngram', '%10s'%'accuracy')\n",
    "for i in range(7,10):\n",
    "    print('%5s%10.2f'%(terms[i][1], mean[i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    C     accuracy\n",
      "  0.1      0.75\n",
      "    1      0.76\n",
      "    5      0.75\n",
      "   10      0.75\n",
      "penalty   accuracy\n",
      "   l1      0.75\n",
      "   l2      0.76\n"
     ]
    }
   ],
   "source": [
    "C=[0.1, 1, 5, 10]\n",
    "p=['l1','l2']\n",
    "clf1 = LogisticRegression(C=C[0], penalty='l2')\n",
    "clf2 = LogisticRegression(C=C[1], penalty='l2')\n",
    "clf3 = LogisticRegression(C=C[2], penalty='l2')\n",
    "clf4 = LogisticRegression(C=C[3], penalty='l2')\n",
    "clf5 = LogisticRegression(C=1, penalty=p[0])\n",
    "clf6 = LogisticRegression(C=1, penalty=p[1])\n",
    "\n",
    "clf_list = [clf1, clf2, clf3, clf4, clf5, clf6]\n",
    "X = X_words_list[1]\n",
    "mean = []\n",
    "std = []\n",
    "arr_accuracies = []\n",
    "\n",
    "for clf in clf_list:\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    for train, test in kf.split(X):\n",
    "        clf.fit(X[train], y[train])\n",
    "        pred = clf.predict(X[test])\n",
    "        accuracies.append(accuracy_score(y[test], pred))\n",
    "    arr_accuracies.append(str(accuracies))\n",
    "    mean.append(np.mean(accuracies))\n",
    "    std.append(np.std(accuracies))\n",
    "\n",
    "print('%5s'%'C', '%12s'%'accuracy')\n",
    "for i in range(0,4):\n",
    "    print('%5s%10.2f'%(C[i], mean[i]))\n",
    "print('penalty', '%10s'%'accuracy')\n",
    "for i in range(0,2):\n",
    "    print('%5s%10.2f'%(p[i], mean[i])) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
